\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{The Jabberwocky: One-shot and few-shot learning of word embeddings}

\author{
  Andrew K. Lampinen\\
  Department of Psychology\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{lampinen@stanford.edu} \\
  \And
  James L. McClelland\\
  Department of Psychology\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{mcclelland@stanford.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about its semantics, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we suggest a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could be used for allowing machine learning systems to learn continually from new words they encounter. 
\end{abstract}

\section{Introduction}

\end{document}
