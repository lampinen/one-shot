\documentclass{article}

\usepackage{iclr2018_conference,times}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}

\title{One-shot and few-shot learning of word embeddings}

\author{
  Andrew K. Lampinen \& James L. McClelland\\
  Department of Psychology\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{\{lampinen,mcclelland\}@stanford.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we draw inspiration from this to highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from the new words they encounter. 
\end{abstract}

\section{Introduction}
Humans are often able to infer approximate meanings of new words from context. For example, consider the following stanza from the poem ``Jabberwocky'' by Lewis Carroll: 
\begin{quote}
\centering
\textit{
He took his vorpal sword in hand:\\
Long time the manxome foe he soughtâ€”\\
So rested he by the Tumtum tree,\\
And stood awhile in thought.}
\end{quote}
Despite the fact that there are several nonsense words, we can follow the narrative of the poem and understand approximately what many of the words mean by how they relate other words. This a vital skill for interacting with the world -- we constantly need to learn new words and ideas from context. Even beyond language, humans are often able adapt quickly to gracefully accomodate situations that differ radically from what they have seen before. Complementary learning systems theory \citep{Kumaran2016} suggests that it is the interaction between a slow-learning system that learns structural features of the world (i.e. a deep-learning like system) and a fast-learning system (i.e. a memory-like system) that allows humans to adapt rapidly from few experiences. \par 
By comparison, standard deep learning systems usually require much more data to learn a concept or task, and sometimes generalize poorly \cite{Lake2016}. They can be trained to learn a concept in one-shot if this is their sole task \cite[e.g.]{Vinyals2016}, but this limits the types of tasks that can be performed. Furthermore, these models typically discard this information after a single use. In order for deep learning systems to be adaptible, they will need to build on their prior knowledge to learn effectively from a few pieces of information. In other words, they will need to integrate learning experiences across different timescales, as complementary learning systems theory suggests that humans and other animals do. In this paper, we explore this broad issue in the specific context of creating a useful representation for a new word based on its context. \par
\subsection{Background}
Continuous representations of words have proven to be very effective \cite[e.g.]{Mikolov2013, Pennington2014}. These approaches represent words as vectors in a space, which are learned from large corpuses of text data. Using these vectors, deep learning systems have achieved success on tasks ranging from natural language translation \cite[e.g.]{Wu2016} to question answering \cite[e.g.]{Santoro2017}. \par
However, these word vectors are typically trained on very large datasets, and there has been surprisingly little prior work on how to learn embeddings for new words once the system has been trained. \citet{Cotterell2016} proposed a method for incorporating morphological information into word embeddings that allows for limited generalization to new words (e.g. generalizing to unseen conjugations of a known verb). However, this is not a general system for learning representations for new words, and requires building in rather strong structural assumptions about the language and having appropriately labelled data to exploit them. \par
More recently \citet{Lazaridou2017} explored multi-modal word learning (similar to what children do when an adult points out a new object), and in the process suggested a simple heuristic for inferring a word vector from context: simply average together all the surrounding word vectors. This is sensible, since the surrounding words will likely occur in similar contexts. However, it ignores all syntactic information, by treating all the surrounding words identically, and it relies on the semantic information being linearly combinable between different word embeddings. Both of these factors will likely limit its performance. \par
Can we do better? A deep learning system which has been trained to perform a language task must have learned a great deal of semantic and syntactic structure which would be useful for inferring and representing the meaning of a new word. However, this knowledge is opaquely encoded in its weights. Is there a way we can exploit this knowledge when learning about a new word? \par
\section{Approach}
We suggest that we already have a way to update the representations of a network while accounting for its current knowledge and inferences -- this is precisely what backpropagation was invented for! Of course, we cannot simply train the whole network to accomodate this new word, this would lead to catastrophic interference. However, \citet{Rumelhart1993} showed that a simple network could be taught about a new input by freezing all its weights except for those connecting the new input to the first hidden layer, and optimizing these by gradient descent as usual. They showed that this resulted in the network making appropriate generalizations about the new input, and by design the training procedure does not interfere with the network's prior knowledge. They used this as a model for human concept learning (as have other authors, for example \citet{Rogers2004}). \par
We take inspiration from this work to guide our approach. To learn from one sentence (or a few) containing a new word, we freeze all the weights in the network except those representing the new word (in a complex NLP system, there may be more than one such set of weights, for example the model we evaluate has distinct input and output embeddings for each word). We then use stochastic gradient descent (\(\eta = 0.01\)) to update the weights for the new word using 100 epochs of training over the sentence(s) containing it. \par
Of course, there are a variety of possible initializations for the embeddings before optimizing. In this paper, we consider three possibilities: 
\begin{enumerate}
\item Beginning with the current embedding for the word (since the model has never seen the word, and thus has been trained to never produce it, this will likely be an embedding that is far from any outputs the network typically produces).
\item Beginning with a vector of zeros.
\item Beginning with the centroid of the other words in the sentence, which \citet{Lazaridou2017} suggested was a useful estimate of an appropriate embedding.
\end{enumerate}
We compare these to two baselines:
\begin{enumerate}
\item The centroid of the embeddings for the other words in the sentence, as in \citet{Lazaridou2017}. 
\item Training the model with the 10 training sentences included in the corpus from the beginning (i.e. the ``standard'' deep-learning approach).
\end{enumerate}
\subsection{Task, Model, and Approach}
The framework we have described for updating embeddings could be applied very generally, but for the sake of this paper we ground it in a simple task: predicting the next word of a sentence based on the previous words, on the Penn Treebank dataset \citep{Marcus1993}. Specifically, we will use the (large) model architecture and approach of \citet{Zaremba2014a}, see Appendix \ref{methods_appdx_model} for details. \par
Of course, human language understanding is much more complex than just prediction, and grounding language in situations and goals is likely important for achieving deeper understanding of language \citep{Gauthier2016}. More recent work has begun to do this \citep[e.g]{Hermann2017}, and it's likely that our approach would be more effective in settings like these. The ability of humans to make rich inferences about text stems from the richness of our knowledge. However, for simplicity, we have chosen to first demonstrate it on the prediction task. \par 
In order to test our one-shot word-learning algorithm on the Penn Treebank, we chose a word which appeared only 20 times in the training set. We removed the sentences containing this word and then trained the model with the remaining sentences for 55 epochs using the learning rate decay strategy of \citet{Zaremba2014a}. Because the PTB dataset contains over 40,000 sentences, the 20 missing ones had essentially no effect on the networks overall performance. We then split the 20 sentences containing the new word into 10 train and 10 test, and tried training on 1 - 10 of them in 10 different permutations (via a balanced Latin square \citep{Campbell1980}, which ensures that each sentence was used for one-shot learning once, and enforces diversity in the multi-shot learning examples). 
\section{Results}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{../../results/10perms_delta_perplexity.png}
\caption{Percent change in perplexity on 10 test sentences containing new word, plotted vs. the number of training sentences, across four different words, comparing optimizing from three different starting points to centroid and training with the word baselines. Averages across 10 permutations are shown in the dark lines, the individual results are shown in light lines. (Note that the full training with the word was only run once, with a single permutation of all 10 training sentences.)}
\label{main_results_1}
\end{figure}
\begin{figure}
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms5_replay_delta_perplexity.png}
\caption{Percent change in perplexity on 10 test sentences containing new word.}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms5_replay_cost_to_others.png}
\caption{Percent change in perplexity on full PTB test corpus.}
\end{subfigure}
\caption{When using a replay buffer, learning new words does not interfere as substantially with prior knowledge.}
\label{ameliorating_interference_fig}
\end{figure}
We first evaluated our approach on the words ``bonuses,'' ``explained,'' ``marketers,'' and ``strategist,'' either initializing from the current embedding for the word (after never seeing it), the zero vector, or the centroid of the surrounding words, and compared to the baselines of just taking the centroid of the surrounding words and full training with the words, see Fig. \ref{main_results_1}. Optimizing from the centroid outperforms all other approaches for learning a new word for all datasets, including the centroid approach of \citet{Lazaridou2017}, and even outperforms full training with the word in three of the four cases (however, this likely comes with a tradeoff, see below). The optimizing approaches are strongly affected by embedding initialization with few training sentences (e.g. one-shot learning), but by 10 sentences they all perform quite similarly. \par
Of course, this learning might still cause interference with the networks prior knowledge. In order to evaluate this, we replicated these findings with four new words (``borrow,'' ``cowboys'', ``immune,'' and ``rice''), but also evaluated the change in perplexity on the PTB test corpus (see Appendix \ref{supp_fig_appdx}, Fig. \ref{interference_fig}). Indeed, we found that while the centroid method does not substantially change the test perplexity on the corpus, the optimizing method causes increasingly more interference as more training sentences are provided, up to a ~1\% decline in the case of training on all 10 sentences. \par
This is not necessarily surprising, the base rate of occurrence of the new word in the training data is artificially inflated relative to its true base rate probability. This problem of learning from data which is locally highly correlated and biased has been solved in many recent domains by the use of replay buffers to interleave learning, especially in reinforcement learning \citep[e.g]{Mnih2015}. The importance of replay is also highlighted in the complementary learning systems theory \citep{Kumaran2016} that helped inspire this work. \par
We therefore tested whether using a replay buffer while learning the new word would ameliorate this interference. Specifically, we sampled 100 negative sentences from the corpus without the word that the network was pre-trained on, and interleaved these at random with the sentences containing the new word (the same negative samples were used for each training epoch). \par
Indeed, interleaving sentences without the word substantially reduced the interference caused by the new word, see Fig. \ref{ameliorating_interference_fig}. This interleaving did not seem to substantially impair learning of the new word. \par
{\color{red} TODO: update figure and paragraphs above + below with new results, at a glance at one run it looks like the interference will be less than 0.05\% at worst}\par
It is interesting to note that only one of these words appears in the PTB test data (``cowboys''), and it only appears once. Why then does full training with the word result in lowered test perplexity on the PTB test data in three out of four cases? This may just be chance variation, of course, but in general we would expect learning about a word to be useful not just for the sake of learning about that word, but because each word is a small piece of the signal by which the network learns about language more generally. Even if a word does not appear in the test corpus, the network may learn something useful from exposure to it.\par 


\subsection{Where is the magic happening?}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{../../results/skipping_by_word.png}
\caption{Comparing optimizing the input embedding, output embedding, or both -- where is the magic happening?}
\label{skipping_results}
\end{figure}
Because the model we are using has distinct input and output embeddings, we are able to evaluate where the benefits of learning about the new word are occurring. Specifically, we compared training where only the softmax weights and bias (output embedding) were learned, to training where only the input embedding was learned, as well as to learning both, for one- and ten-shot learning. See Fig. \ref{skipping_results} for our results.\par
We found that it was mostly the output embeddings that are improving. In one-shot learning, changing the input embedding alone causes almost no improvement, and changing both embeddings does not see substantially different from changing just the output embedding. However, with ten training sentences the updated input embedding is producing some improvement, both alone and when trained together with the output embedding. Even in this case, however, the effect of the input embedding is still much smaller than the effect of the output embedding. That is, the model is mostly improving at predicting the new word in context, rather than predicting context based on the new word. \par 
This is sensible for several reasons. First, whatever the new word conveys about the context will already be partly conveyed by the other words in the context. Second, our training approach was more unnatural than the situations in which a human might experience a new word, or even than the pretraining for the network, in that the sentences were presented without the context of surrounding sentences. This means that the model has less data to learn about how the new word predicts surrounding context, and less information about the context which predicts this word. This may also explain why full training with the word still produced better results in some cases than updating for it.\par
\subsection{Digging deeper into model performance} \label{digging_deeper}
\begin{table}[ht]
\centering
\begin{tabular}{|l|rrr|}
  \hline
  & New word is correct & Wrong but relevant & Wrong and irrelevant \\ 
  \hline
  Full training with the word & -9.21 & -12.75 & -15.13 \\ 
  Centroid & -9.16 & -9.46 & -10.44 \\ 
  Optimizing from centroid & -6.20 & -9.32 & -10.91 \\ 
   \hline
\end{tabular}
\caption{Average log-probabilities of new word when: the word is the current target, the new word is not the current target but does appear in the current sentence, and the word doesn't appear in the sentence or context. (Analysis computed with 10 training sentences, patterns are similar but less severe with fewer sentences, see Appendix \ref{log_prob_appendix}.)} \label{word_prob_table} 
\end{table}
Of course, as the above results illustrate, simply looking at change in perplexity as a measure of success can be difficult to interpret. Thus we conducted more detailed analyses of the model's predictions. Since we know that most of the benefit of the one-shot training in our model comes from predicting the new word in context, we evaluated how well the word was predicted in three cases: when it was the actual target word, when it was not the current target but did appear in the sentence, and in ``irrelevant'' sentences. This allowed us to investigated whether the model was learning something useful, or simply overfitting to the new data. We compared the average log-probability assigned to the new word after the softmax in each of these cases for the full training with the word baseline, the centroid approach to learning from 10 words, and our approach. The relevant cases were evaluated on our held-out test data-set for that word, the irrelevant was evaluated on the first 10 sentences of the PTB test corpus (which came from an article that does not contain any of the words we used). See Table \ref{word_prob_table} for our results. \par 
The model fully trained with the word shows clear distinctions between the three conditions -- the word is estimated to be about 10 times more likely in contexts where it appears than in irrelevant contexts, and is estimated to be about 25 times more likely again when it actually appears. However, the model severely underestimates the probability of the word when it does appear; the word would have a similar log probability under a uniform distribution over the whole vocabulary. The centroid method also has this issue, but in addition it does not even distinguish particularly well between contexts. The word is only estimated to be about 4 times more likely when it is the target than in irrelevant contexts. \par
By contrast, our approach results in a good distinction between contexts -- the word is predicted to be about 5 times as likely in context where it appears compared to the irrelevant context, and about 25 times as likely again when the word actually appears. These relative probabilities are quite similar to those exhibited by the model fully trained with the word. Furthermore, the optimized embedding is much more successful at predicting the word when it appears. In all these respects, it appears superior to the centroid approach. When compared to the full training with the word, however, it appears that the base rate estimates of the prevalence of the word are inflated (which is sensible, even with 100 negative samples per positive sample in our training data the prevalence of the word is much higher than in the actual corpus). This explains the residual small increase in test perplexity on the dataset not containing the new word. It is possible that this could be ameliorated either by setting a prior on the bias for the new (perhaps penalize the \(\ell_2\) norm of the distance of the bias from the values for other rare words), or with a validation set, or just by using more negative samples during training. In any case, it appears that the optimized embeddings are capturing some of the important features of when the word does and does not appear, and are doing so more effectively than the centroid approach.
\subsection{Embedding similarity analyses}
It has often been noted that relationships between word embeddings capture semantic features of the words, such as analogies between words \citep{Mikolov2013}. Thus it is natural to ask to what extent the word vectors produced by one-shot learning produce similarity structures close to those produced by full training with the word. To address this question, we computed the dot product\footnote{This seems to be the appropriate similarity ``metric'', given that the logits for the softmax over the vocabulary are formed by a matrix-vector multiplication which amounts to computing these dot products.} of the new word output embedding with all other word output embeddings. This vector of dot-product results can be thought of as a similarity map for the new word. We then compared these similarity maps to the similarity maps produced by full training with the word by computing correlations between the similarity maps. (See Appendix \ref{supp_fig_appdx} Fig. \ref{RSA_results} for our results.)\par
Different runs of full training with the word produced very correlated similarity structures (all correlations greater than 0.9). Thus there does appear to be some consistent semantic structure that the embeddings are capturing. However, neither the centroid nor the optimizing one-shot learning methods seemed to be capturing this structure. The centroid method actually produced similarity structures that were \textbf{negatively} correlated with the full training similarity structures in three out of four cases! The optimizing method produced slightly more similar structures, but only in so far as most of its similarity structures were effectively uncorrelated with the full training similarity structure. Only for one word each did the methods produce a similarity structure that was strongly positively correlated with the structure produced by full training with the word. \par
These results are difficult to interpret. On the one hand, the consistency of the semantic knowledge produced by the network when fully trained with the word suggests that it is discovering important structure. On the other hand, the later learning approaches actually produce even more internally consistency, at least when training with several sentences. Full training with the word produces an average correlation of 0.93 between different training runs, whereas optimizing with all ten sentences produces an average correlation of 0.97 (and optimizing with 1 sentence produces an average correlation of 0.50). Furthermore, the results of section \ref{digging_deeper} above suggest that our approach is doing about as well at distinguishing contexts where the new word does and does not appear. That is, the optimizing runs are extracting consistent structure as well, they are just creating different representational structures than the full training runs. \par
These different representations may be explained by the removal of some contextual cues when the sentences are presented in isolation (noted above) -- the networks doing full training with the word are able to pay attention to features across sentence boundaries, which may give them exposure to word co-occurrences that the one-shot learning approach is missing. This could be solved in real contexts by just including the surrounding context sentences in the training, and this might result in more similar representational structure between full training with the word and learning the word later. \par
\section{Discussion}
Overall, using our technique of updating only the embedding vectors of a word while training on sentences containing it and negative sampled sentences from the networks past experience seems quite effective. It allows for substantial reductions in perplexity on text containing the new word, without greatly interfering with knowledge about other words. Furthermore, it seems to be capturing more useful structure about how the word is used in context than previous approaches, and performs close to as well as full training with the word. These results are exciting beyond their potentiatl applications to natural language processing -- this technique could easily be extended to adapting systems to other types of new experiences, for example a vision network for an RL agent could have a few new filters per layer added and trained to accomodate a new type of object. \par 
Under what circumstances will this strategy fail? Complementary learning systems theory \citep{Kumaran2016}, from which we drew inspiration, suggests that information which is \emph{schema-consistent} (i.e. fits in with the network's previous knowledge) can be integrated easily, whereas \emph{schema-inconsistent} knowledge (i.e. knowledge that differs from the network's previous experience) will cause interference. Similar principles should apply here. Our approach should work for learning a new word on a topic which is already somewhat familiar, but would likely fail to learn from a new word in a context that is not well understood. For example, it would be difficult to learn a new German word from context if the model has only experienced English language. \par  
On the other hand, this perspective also provides promises. We expect that our technique would perform even better in a system that had a more sophisticated understanding of language, because it would have more prior knowledge from which to bootstrap understanding of new words. Thus it would be very interesting to apply our technique on more complicated tasks like question answering, such as \citet{Santoro2017}, or in a grounded context, such as \citet{Hermann2017}. \par 
\subsection{Word-by-word differences}
We have presented all the results in this paper broken down by word rather than as averages, because there were large word-by-word differences on almost every analysis. Full training with the word did not improve the new word test perplexity more than the centroid approach on most of the words, but on two of them (``immune'' and ``strategist'') it did substantially better than even optimizing. Similarly, the optimizing method produced representational similarity structures that were more correlated with the full training with the word structure on three of the words we tested, but on a fourth the centroid approach produced a higher correlation. These results are likely due to the complex interactions between the context and the network's prior knowledge, and are difficult to quantify exactly. \par
However, variable results are not a unique feature of learning the words later -- even full training with the word produced much better results on some words than others. Especially when considering small slices of a dataset (like the handful out of 42,000 sentences that contained the words we considered), the data can be quite noisy. Furthermore, it's likely that some types of words are easier to learn than others (e.g. verbs may have more complex interactions with their context than nouns do). Thus these diverse patterns in our results should not be interpreted as a unique deficiency in our technique. Even standard deep learning is fallible when evaluated on its ability to learn from small amounts of data embedded in a larger dataset. \par
\section{Conclusions}
We have presented a technique for doing one- or few-shot learning of word embeddings from text data: freeze all the weights in the network except the embeddings for the new word, and then optimize these embeddings for the sentence, interleaving with negative examples from network's prior experience and stopping early. This results in substantial improvement of the ability to predict the word in context, with minimal impairment of prediction of other words. This technique could allow natural language processing systems to adapt more flexibly to a changing world, like humans do. More generally, it could serve as a model for how to integrate rapid adaption into deep learning systems.  

\bibliographystyle{iclr2018_conference}
\bibliography{one_shot_words}

\appendix
\section{Supplementary figures} \label{supp_fig_appdx}

\begin{figure}
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms2_delta_perplexity.png}
\caption{Percent change in perplexity on 10 test sentences containing new word.}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms2_cost_to_others.png}
\caption{Percent change in perplexity on full PTB test corpus.}
\end{subfigure}
\caption{Interference with prior knowledge caused by na\"{i}ve use of our approach}
\label{interference_fig}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../../results/10perms4_similarity_correlations.png}
\caption{Similarity of representational similarity: how correlated are the similarity structures generated by the different methods with the similarity structure produced by training with the word?}
\label{RSA_results}
\end{figure}

\section{Log-probability analysis broken out} \label{log_prob_appendix}
% latex table generated in R 3.4.1 by xtable 1.8-2 package
% Mon Oct 23 20:24:49 2017
\begin{table}[ht]
\centering
\begin{tabular}{lrlrrr}
  \hline
new\_word & num\_train & Approach & New word is correct & Wrong and irrelevant & Wrong but relevant \\ 
  \hline
borrow &   1 & centroid & -9.75 & -10.43 & -9.65 \\ 
  borrow &   1 & opt\_centroid & -6.31 & -10.10 & -8.51 \\ 
  borrow &   5 & centroid & -9.69 & -10.42 & -9.64 \\ 
  borrow &   5 & opt\_centroid & -5.78 & -10.44 & -8.71 \\ 
  borrow &  10 & centroid & -9.65 & -10.42 & -9.61 \\ 
  borrow &  10 & opt\_centroid & -6.46 & -11.03 & -9.73 \\ 
  borrow &  10 & with\_word & -8.74 & -15.31 & -13.21 \\ 
  cowboys &   1 & centroid & -9.33 & -10.40 & -9.06 \\ 
  cowboys &   1 & opt\_centroid & -7.57 & -8.88 & -8.84 \\ 
  cowboys &   5 & centroid & -9.26 & -10.43 & -9.09 \\ 
  cowboys &   5 & opt\_centroid & -6.27 & -11.21 & -9.06 \\ 
  cowboys &  10 & centroid & -9.26 & -10.42 & -9.10 \\ 
  cowboys &  10 & opt\_centroid & -6.23 & -11.10 & -9.71 \\ 
  cowboys &  10 & with\_word & -8.47 & -15.63 & -12.75 \\ 
  immune &   1 & centroid & -9.15 & -10.53 & -9.63 \\ 
  immune &   1 & opt\_centroid & -8.99 & -9.17 & -9.39 \\ 
  immune &   5 & centroid & -9.00 & -10.47 & -9.49 \\ 
  immune &   5 & opt\_centroid & -7.72 & -9.90 & -9.05 \\ 
  immune &  10 & centroid & -9.01 & -10.50 & -9.49 \\ 
  immune &  10 & opt\_centroid & -6.23 & -11.03 & -8.96 \\ 
  immune &  10 & with\_word & -9.85 & -14.17 & -12.41 \\ 
  rice &   1 & centroid & -8.67 & -10.44 & -9.64 \\ 
  rice &   1 & opt\_centroid & -7.65 & -10.09 & -8.80 \\ 
  rice &   5 & centroid & -8.66 & -10.43 & -9.61 \\ 
  rice &   5 & opt\_centroid & -6.35 & -10.20 & -8.14 \\ 
  rice &  10 & centroid & -8.66 & -10.42 & -9.61 \\ 
  rice &  10 & opt\_centroid & -5.85 & -10.47 & -8.64 \\ 
  rice &  10 & with\_word & -9.84 & -15.43 & -12.44 \\ 
   \hline
\end{tabular}
\end{table}

\section{Implementation details} \label{methods_appdx}
\subsection{Model} \label{methods_appdx_model}
We used the ``large'' model described by \citet{Zaremba2014a}, and use all their hyper-parameters for the pre-training. Specifically, the model consists of 2 layers of stacked LSTMs with a hidden size of 1500 units, 35 recurrent steps, and dropout (\(p_{keep} = 0.35\)) applied to the non-recurrent connections. The gradients were clipped to a max global norm of 10. Weights were initialized uniformly from \([-0.04, 0.04]\). \par
\subsubsection{Training on full corpus}
The loss function was the cross-entropy loss of the model predictions. The model was trained for 55 epochs by stochastic gradient descent with a batch size of 20. The learning rate was set to 1 for the first 14 epochs, and then was decayed by a multiplier of \(1 / 1.15\) per epoch for the remainder of training. \par 
\subsubsection{Training on a new word} 
\textbf{Centroid:} We extracted the input embeddings, softmax weights, and softmax biases of every word in the sentence except for the new word. We then averaged together these input embeddings to produce the new input embedding for the new word, etc. \par
\textbf{Optimizing:} We initialized the new input embedding in one of three ways: with the centroid from above, with a vector of zeros, or with the current embedding vector for the word (note that this last would not be possible in a general context where a new word is not ``expected''). Starting from this point, we ran 100 epochs of batch gradient descent (batch size was equal to the number of training sentences), with a learning rate of 0.01. As a loss, we used the cross entropy loss plus 0.01 times the \(\ell_2\) norm of the new embedding.\par 
\end{document}
