\documentclass{article}

\usepackage{iclr2018_conference,times}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}

\title{Jabberwocky: One-shot and few-shot learning of word embeddings}

\author{
  Andrew K. Lampinen \& James L. McClelland\\
  Department of Psychology\\
  Stanford University\\
  Stanford, CA 94305 \\
  \texttt{\{lampinen,mcclelland\}@stanford.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Standard deep learning systems require thousands or millions of examples to learn a concept, and cannot integrate new concepts easily. By contrast, humans have an incredible ability to do one-shot or few-shot learning. For instance, from just hearing a word used in a sentence, humans can infer a great deal about it, by leveraging what the syntax and semantics of the surrounding words tells us. Here, we highlight a simple technique by which deep recurrent networks can similarly exploit their prior knowledge to learn a useful representation for a new word from little data. This could make natural language processing systems much more flexible, by allowing them to learn continually from new words they encounter. 
\end{abstract}

\section{Introduction}
Humans are often able to infer approximate meanings of new words from context. For example, consider the following stanza from the poem ``Jabberwocky'' by Lewis Carroll: 
\begin{quote}
\centering
\textit{
He took his vorpal sword in hand:\\
Long time the manxome foe he soughtâ€”\\
So rested he by the Tumtum tree,\\
And stood awhile in thought.}
\end{quote}
Despite the fact that there are several nonsense words, we can follow the narrative of the poem and understand approximately what many of the words mean by how they relate other words. This a vital skill for interacting with the world -- we constantly need to learn new words and ideas from context. Even beyond language, humans are often able adapt quickly to gracefully accomodate situations that differ radically from what they have seen before. Complementary learning systems theory \citep{Kumaran2016} suggests that it is the interaction between a slow-learning system that understands the task (i.e. a deep-learning like system) and a fast-learning system (i.e. a memory-like system) that allows humans to adapt rapidly from few experiences. \par 
Compared to humans, standard deep learning systems usually require much more data to learn a concept or task, and sometimes generalize poorly \cite{Lake2016}. They can be trained to learn a concept in one-shot if this is their sole task \cite[e.g.]{Vinyals2016}, but these models simply discard this information after a single use. Can deep learning systems performing more complex tasks build on their prior knowledge to learn effectively from a few pieces of information? In other words, how can they integrate learning experiences across different timescales? In this paper, we explore this issue in the specific context of creating a useful representation for a new word based on its context. \par
\subsection{Background}
Continuous representations of words have proven to be very effective \cite[e.g.]{Mikolov2013, Pennington2014}. These approaches represent words as vectors in a space, which are learned from large corpuses of text data. Using these vectors, deep learning systems have achieved success on tasks ranging from natural language translation \cite[e.g.]{Wu2016} to question answering \cite[e.g.]{Santoro2017}. \par
However, these word vectors are typically trained on very large datasets, and there has been surprisingly little prior work on how to learn embeddings for new words once the system has been trained. \citet{Cotterell2016} proposed a method for incorporating morphological information into word embeddings that allows for limited generalization to new words (e.g. generalizing to unseen conjugations of a known verb). However, this is not a general system for learning representations for new words, and requires building in rather strong structural assumptions about the language and having appropriately labelled data to exploit them. \par
More recently \citet{Lazaridou2017} explored multi-modal word learning (similar to what children do when an adult points out a new object), and in the process suggested a simple heuristic for inferring a word vector from context: simply average together all the surrounding word vectors. This is sensible, since the surrounding words will likely occur in similar contexts. However, it ignores all syntactic information, by treating all the surrounding words identically, and it relies on the semantic information being linearly combinable between different word embeddings. Both of these factors will likely limit its performance. \par
Can we do better? A deep learning system which has been trained to perform a language task must have learned a great deal of semantic and syntactic structure which would be useful for inferring the meaning of a new word, and representing it in a way which is useful and does not interfere with the networks prior knowledge. However, this knowledge is opaquely encoded in its weights. Is there a way we can exploit this knowledge when learning about a new word? \par
\section{Approach}
We suggest that we already have a way to update the representations of a network while accounting for its current knowledge and inferences -- this is precisely what backpropagation was invented for! Of course, we cannot simply train the whole network to accomodate this new word, this would lead to catastrophic interference. However, \citet{Rumelhart1993} showed that a simple network could be taught about a new input by freezing all its weights except for those connecting the new input to the first hidden layer, and optimizing these by gradient descent as usual. They showed that this resulted in the network making appropriate generalizations about the new input, and by design the training procedure does not interfere with the network's prior knowledge. They used this as a model for human concept learning (as have other authors, for example \citet{Rogers2004}). \par
We take inspiration from this work to guide our approach. To learn from one sentence (or a few) containing a new word, we freeze all the weights in the network except those representing the new word (in a complex NLP system, there may be more than one such set of weights, for example the model we evaluate has distinct input and output embeddings for each word). We then use stochastic gradient descent (\(\eta = 0.01\)) to update the weights for the new word using 100 epochs of training over the sentence(s) containing it. \par
Of course, there are a variety of possible initializations for the embeddings before optimizing. In this paper, we consider three possibilities: 
\begin{enumerate}
\item Beginning with the current embedding for the word (since the model has never seen the word, and thus has been trained to never produce it, this will likely be an embedding that is far from any outputs the network typically produces).
\item Beginning with a vector of zeros.
\item Beginning with the centroid of the other words in the sentence, which \citet{Lazaridou2017} suggested was a useful estimate of an appropriate embedding.
\end{enumerate}
We compare these to two baselines:
\begin{enumerate}
\item The centroid of the embeddings for the other words in the sentence, as in \citet{Lazaridou2017}. 
\item Training the model with the 10 training sentences included in the corpus from the beginning (i.e. the ``standard'' deep-learning approach).
\end{enumerate}
\subsection{Task, Model, and Approach}
The framework we have described for updating embeddings could be applied very generally, but for the sake of this paper we ground it in a simple task: predicting the next word of a sentence based on the previous words, on the Penn Treebank dataset \citep{Marcus1993}. Specifically, we will use the (large) model architecture and approach of \citet{Zaremba2014a}. \par
Of course, human language understanding is much more complex than just prediction, and grounding language in situations and goals is likely important for achieving deeper understanding of language \citep{Gauthier2016}. More recent work has begun to do this \citep[e.g]{Hermann2017}, and it's likely that our approach would be more effective in settings like these. The ability of humans to make rich inferences about text stems from the richness of our knowledge. However, for simplicity, we have chosen to first demonstrate it on the prediction task. \par 
In order to test our one-shot word-learning algorithm on the Penn Treebank, we chose a word which appeared only 20 times in the training set. We removed the sentences containing this word and then trained the model with the remaining sentences for 55 epochs using the learning rate decay strategy of \citet{Zaremba2014a}. Because the PTB dataset contains over 40,000 sentences, the 20 missing ones had essentially no effect on the networks overall performance. We then split the 20 sentences containing the new word into 10 train and 10 test, and tried training on 1 - 10 of them in 10 different permutations (via a balanced Latin square \citep{Campbell1980}, which ensures that each sentence was used for one-shot learning once, and enforces diversity in the multi-shot learning examples). 
\section{Results}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{../../results/10perms_delta_perplexity.png}
\caption{Percent change in perplexity on 10 test sentences containing new word, plotted vs. the number of training sentences, across four different words, comparing optimizing from three different starting points to centroid and training with the word baselines. Averages across 10 permutations are shown in the dark lines, the individual results are shown in light lines. (Note that the full training with the word was only run once, with a single permutation of all 10 training sentences.)}
\label{main_results_1}
\end{figure}
We first evaluated our approach on the words ``bonuses,'' ``explained,'' ``marketers,'' and ``strategist,'' either initializing from the current embedding for the word (after never seeing it), the zero vector, or the centroid of the surrounding words, and compared to the baselines of just taking the centroid of the surrounding words and full training with the words, see Fig. \ref{main_results_1}. Optimizing from the centroid was clearly the best approach, it outperforms all other approaches for learning a new word for all datasets, including the centroid approach \citet{Lazaridou2017}, and even outperforms full training with the word in three of the four cases (however, this likely comes with a tradeoff, see below). The optimizing approaches are strongly affected by embedding initialization with few training sentences (e.g. one-shot learning), but by 10 sentences they all perform quite similarly. \par
Of course, this learning might still cause interference with the networks prior knowledge. In order to evaluate this, we replicated these findings with four new words (``borrow,'' ``cowboys'', ``immune,'' and ``rice''), but also evaluated the change in perplexity on the PTB test corpus (see Appendix \ref{supp_fig_appdx}, Fig. \ref{interference_fig}). Indeed, we found that while the centroid method does not substantially change the test perplexity on the corpus, the optimizing method causes increasingly more interference as more training sentences are provided, up to a ~1\% decline in the case of training on all 10 sentences. \par
This is not necessarily surprising, the base rate of occurrence of the new word in the training data is artificially inflated relative to its true base rate probability. This problem of learning from data which is locally highly correlated and biased has been solved in many recent domains by the use of replay buffers to interleave learning, especially in reinforcement learning \citep[e.g]{Mnih2015}, an idea that also relates to the complementary learning systems theory \citep{Kumaran2016} that helped inspire this work. \par
We therefore tested whether using a replay buffer while learning the new word would ameliorate this interference. Specifically, we sampled 100 negative sentences from the corpus without the word that the network was pre-trained on, and interleaved these at random with the sentences containing the new word (the same negative samples were used for each training epoch). \par
\begin{figure}
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms5_replay_delta_perplexity.png}
\caption{Percent change in perplexity on 10 test sentences containing new word.}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms5_replay_cost_to_others.png}
\caption{Percent change in perplexity on full PTB test corpus.}
\end{subfigure}
\caption{When using a replay buffer, learning new words does not interfere as substantially with prior knowledge.}
\label{ameliorating_interference_fig}
\end{figure}
Indeed, interleaving sentences without the word substantially reduced the interference caused by the new word, see Fig. \ref{ameliorating_interference_fig}. This interleaving did not seem to substantially impair learning of the new word. \par
{\color{red} TODO: update figure and paragraphs above + below with new results, at a glance at one run it looks like the interference will be less than 0.05\% at worst}\par
It is interesting to note that only one of these words appears in the PTB test data (``cowboys''), and it only appears once. Why then does full training with the word result in lowered test perplexity on the PTB test data in three out of four cases? This may just be chance variation, of course, but in general we would expect learning about a word to be useful not just for the sake of learning about that word, but because each word is a small piece of the signal by which the network learns about language more generally. Even if a word does not appear in the test corpus, the network may learn something useful from exposure to it.\par 


\subsection{Where is the magic happening?}
\begin{figure}
\centering
\includegraphics[width=\textwidth]{../../results/skipping_by_word.png}
\caption{Comparing optimizing the input embedding, output embedding, or both -- where is the magic happening?}
\label{skipping_results}
\end{figure}
Because the model we are using has distinct input and output embeddings, we are able to compare learning these individually to evaluate where the benefits of learning about the new word are occurring. Specifically, we compared training where only the softmax weights and bias (output embedding) for the new word were changed to training where only the input embedding for the new word was changed, as well as to changing both embeddings as above, for one-shot and ten-shot learning. See Fig. \ref{skipping_results} for our results.\par
We found that it was mostly the output embeddings that are improving. In one-shot learning, changing the input embedding alone causes almost no improvement, and changing the both embeddings does not see substantially different from changing just the output embedding. However, with ten training sentences the updated input embedding is producing some improvement, both alone and when trained together with the output embedding. Even in this case, however, the effect of the input embedding is still much smaller than the effect of the output embedding. That is, the model is mostly improving at predicting the new word in context, rather than predicting context based on the new word. \par 
This is sensible for several reasons. First, whatever the new word conveys about the context will already be partly conveyed by the other words in the context. Second, our training approach was more unnatural than the situations in which a human might experience a new word, or even than the pretraining for the network, in that the sentences were presented without the context of surrounding sentences. This means that the model has less data to learn about how the new word predicts surrounding context, and less information about the context which predicts this word. This may also explain why full training with the word still produced better results in some cases than updating for it.\par

\subsection{Embedding similarity analyses}
{\color{red} TODO and integrate into discussion}

\section{Discussion}
Overall, using our technique of updating only the embedding vectors of a word while training on sentences containing it and negative sampled sentences from the networks past experience seems quite effective. It allows for substantial reductions in perplexity on text containing the new word, without greatly interfering with knowledge about other words. This technique could easily be extended to adapting systems to other types of new experiences, for example a vision network for an RL agent could have a few new filters per layer added and trained to accomodate a new type of object. \par  
Under what circumstances will this strategy fail? Complementary learning systems theory \citep{Kumaran2016}, from which we drew inspiration, suggests that information which is \emph{schema-consistent} (i.e. fits in with the network's previous knowledge) can be integrated easily, whereas \emph{schema-inconsistent} knowledge (i.e. knowledge that differs from the network's previous experience) will cause interference. Similar principles should apply here. Our approach should work for learning a new word on a topic which is already known, but would likely fail to learn from a new word in a context that is not well understood. For example, it would be difficult to learn a new German word from context if the model has only experienced English language. \par  
On the other hand, this perspective also provides promises. We expect that our technique would perform even better in a language system that had a more sophisticated understanding of language, because it will have more prior knowledge from which it can bootstrap understanding. Thus it would be very interesting to apply our technique in a system using language on more complicated tasks like question answering, such as \citet{Santoro2017}, or in a grounded context, such as \citet{Hermann2017}. \par 
\section{Conclusions}
We have presented a technique for doing one- or few-shot learning of word embeddings from text data: freeze all the weights in the network except the embeddings for the new word, and then optimize these embeddings for the sentence, interleaving with negative examples from network's prior experience and stopping early. This results in substantial improvement of the ability to predict the word in context, with minimal impairment of prediction of other words. This technique could allow natural language processing systems to adapt more flexibly to a changing world, like humans do. More generally, it could serve as a model for how to integrate rapid adaption into deep learning systems.  

\bibliographystyle{iclr2018_conference}
\bibliography{one_shot_words}

\appendix
\section{Supplementary figures} \label{supp_fig_appdx}

\begin{figure}
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms2_delta_perplexity.png}
\caption{Percent change in perplexity on 10 test sentences containing new word.}
\end{subfigure}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{../../results/10perms2_cost_to_others.png}
\caption{Percent change in perplexity on full PTB test corpus.}
\end{subfigure}
\caption{Interference with prior knowledge caused by na\"{i}ve use of our approach}
\label{interference_fig}
\end{figure}
\end{document}
